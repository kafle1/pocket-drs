PocketDRS – Online Architecture Master Plan (Flutter + Python Server)
Last updated: 2025-12-24
Owner: Niraj Kafle

0) What this plan is
- This plan replaces the “fully offline/on-device” assumption with an ONLINE architecture.
- Flutter becomes a thin client: capture/import video, collect user inputs (segment, taps), upload to server, show results.
- Python server performs heavy processing: frame extraction, ball tracking, calibration, (optional) 3D/physics, and LBW assessment.

Important note (engineering reality):
- A Python server is not strictly required, but it IS a pragmatic way to use mature CV tooling (OpenCV, NumPy/SciPy) and to iterate faster.
- Online introduces new constraints: latency, upload time, privacy, failure modes, security.


1) Product definition (what “DONE” means)

1.1 MVP (must ship)
- Import or record a cricket delivery video on Android.
- Choose analysis segment (release → impact) on device.
- Calibrate pitch plane using either:
  A) user taps 4 pitch corners (current approach), OR
  B) marker-based calibration (optional MVP+, but recommended).
- Run server-side analysis and return:
  - 2D ball track points over time (image pixels + confidence)
  - pitch-plane mapped track (meters)
  - bounce estimate (or allow manual bounce selection)
  - impact estimate (or manual impact override)
  - wicket line prediction (pitch-plane) with confidence/diagnostics
  - LBW decision summary (prototype-grade)
- Display results in Flutter:
  - video overlay with tracked points
  - top-down pitch view + wicket prediction
  - human-readable explanation (what was assumed, what failed, what to fix)
- Never crash. All failures should be handled and explained.

1.2 MVP+ (nice-to-have)
- Robust ball detector/tracker (tracking-by-detection):
  - classical CV first; optionally upgrade to ML later.
- Marker-based calibration (AprilTag/ArUco) with quality score.
- Export run artifacts (JSON + screenshots) for report/demo.
- Background job queue + progress reporting.

1.3 Stretch (proposal-aligned)
- Monocular 3D trajectory approximation with constraints and simple physics.
- Edge detection via audio (single-device), then dual-device sync.


2) Architecture overview

2.1 Client (Flutter)
Responsibilities:
- Acquire video (record/import).
- Gather user inputs:
  - segment times
  - optional pitch corner taps
  - optional seed point(s) for ball detection (fallback)
  - optional manual bounce/impact correction
- Upload video + metadata to backend.
- Poll for job completion (or use WebSocket).
- Render results (existing UI can be reused): ReviewScreen, AnalysisScreen, LBW review.

2.2 Server (Python)
Responsibilities:
- Receive video uploads.
- Extract frames or decode at requested timestamps.
- Track ball across frames.
- Compute calibration transform (homography / PnP).
- Compute pitch-plane track.
- Compute LBW assessment and return structured results.
- Store artifacts (logs, intermediate images) for debugging.

2.3 Deployment model options
Option A (recommended for FYP reliability): Local network “online”
- Run Python server on laptop (same Wi‑Fi).
- Phone app uploads to laptop server.
- Pros: fast iteration, privacy, no cloud costs.
- Cons: requires laptop on demo day.

Option B: Cloud hosted
- Server on VPS (Render/Fly.io/AWS).
- Pros: no laptop needed.
- Cons: cost, latency, privacy concerns, harder debugging.

We will design so Option A and B share the same API.


3) Data contracts (API design)

3.1 Conventions
- All responses are JSON.
- Every job has:
  - job_id (string)
  - status: queued | running | succeeded | failed
  - error: null or { code, message, details }
- All coordinates:
  - image pixels: (x_px, y_px) floats
  - pitch plane meters: (x_m, y_m) floats with a defined coordinate system

3.2 Endpoints (v1)

POST /v1/jobs
- Purpose: create an analysis job
- Request (multipart/form-data):
  - video_file: mp4
  - request_json: JSON string

request_json structure (example):
{
  "client": {"platform": "android", "app_version": "x.y.z"},
  "video": {"source": "import" | "record", "rotation_deg": 0},
  "segment": {"start_ms": 1234, "end_ms": 5678},
  "calibration": {
    "mode": "taps" | "marker" | "none",
    "pitch_corners_px": [ {"x":..,"y":..}, ...4 ],
    "pitch_dimensions_m": {"length": 20.12, "width": 3.05}
  },
  "tracking": {
    "mode": "auto" | "seeded",
    "seed_px": {"x":..,"y":..},
    "max_frames": 180,
    "sample_fps": 60
  },
  "overrides": {
    "bounce_index": null,
    "impact_index": null,
    "full_toss": false
  }
}

- Response:
{ "job_id": "...", "status": "queued" }

GET /v1/jobs/{job_id}
- Purpose: poll job status and summary
- Response:
{ "job_id": "...", "status": "running", "progress": {"pct": 45, "stage": "tracking"} }

GET /v1/jobs/{job_id}/result
- Purpose: fetch final result
- Response:
{
  "job_id": "...",
  "status": "succeeded",
  "result": {
    "video": {"duration_ms": 12345, "fps_est": 60.0},
    "diagnostics": {"warnings": [...], "log_id": "..."},
    "track": {
      "points": [
        {"t_ms": 0, "x_px": 10.1, "y_px": 20.2, "confidence": 0.91},
        ...
      ]
    },
    "calibration": {
      "mode": "taps",
      "homography": [[...],[...],[...]],
      "quality": {"score": 0.82, "notes": [...]}
    },
    "pitch_plane": {
      "points_m": [ {"t_ms": 0, "x_m": 1.2, "y_m": 0.3}, ... ]
    },
    "events": {
      "bounce": {"index": 42, "confidence": 0.6},
      "impact": {"index": 85, "confidence": 0.7}
    },
    "lbw": {
      "likely_out": true,
      "checks": {
        "pitching_in_line": true,
        "impact_in_line": true,
        "wickets_hitting": true
      },
      "prediction": {"x_at_stumps_m": 0.12, "band_half_width_m": 0.1143}
    }
  }
}

GET /v1/jobs/{job_id}/artifacts/{name}
- Purpose: fetch debug artifacts (optional)
- Examples:
  - overlay.png
  - topdown.png
  - debug_track.json


4) Server implementation plan (Python)

4.1 Tech choices
- Framework: FastAPI
- Video decode: OpenCV (cv2.VideoCapture) OR PyAV (more robust).
- Math: NumPy
- Optional: SciPy for smoothing/fitting
- Background jobs: Celery/RQ OR built-in background tasks + simple in-memory queue for MVP.
- Storage:
  - MVP local: filesystem under ./data/jobs/{job_id}/
  - Cloud: object storage (S3-compatible)

4.2 Processing pipeline (MVP)
Stage A: ingest
- Validate request_json.
- Save upload to job folder.
- Extract basic metadata: duration, fps estimate, frame count.

Stage B: frame sampling
- Determine timestamps from segment + sample_fps.
- Decode frames; if decode fails at a timestamp:
  - fallback to nearest valid frame
  - record warning with details
- Persist a few key frames for debugging.

Stage C: ball tracking
- Start simple and stable:
  - seeded mode: user seed in first frame
  - detect by color/intensity + local search window
  - track with Kalman filter
  - confidence based on detection residuals
- Auto mode can be added later.

Stage D: calibration
- Taps mode:
  - compute homography from 4 corners
  - validate geometry; if degenerate, return a structured error and suggest retapping
- Marker mode (MVP+): detect tag corners and compute pose/homography

Stage E: pitch-plane mapping
- Transform each pixel point through homography into meters.
- Remove outliers (NaN, spikes) and smooth.

Stage F: events + LBW assessment (prototype-grade)
- Determine bounce index:
  - heuristic based on curvature / y-velocity sign change OR allow manual override
- Determine impact:
  - default to last good point, but allow manual override from client
- Predict x_at_stumps:
  - fit line on last N points pre-impact
- Compute checks:
  - pitching-in-line, impact-in-line, wickets-hitting
- Return full diagnostics.

4.3 Error handling policy
- Never crash the worker process due to a single bad video.
- All errors become typed API errors:
  - VIDEO_DECODE_FAILED
  - INVALID_SEGMENT
  - CALIBRATION_DEGENERATE
  - TRACKING_LOST
  - INTERNAL_ERROR
- Every error includes:
  - message for user
  - details for developer (timestamps, stack trace id)


5) Flutter changes (migration path)

5.1 Minimal invasive approach
Reuse your existing UI flow:
- HomeScreen: record/import (same)
- ReviewScreen: segment selection (same)
- PitchCalibrationScreen: taps (same)
- BallSeedScreen: seed point (keep as fallback)
- AnalysisScreen: instead of running BallTracker locally:
  - upload video + request_json
  - show progress UI
  - fetch results
- LbwReviewScreen: use server-provided pitch-plane points OR reuse existing assessor using returned track.

5.2 New modules needed in Flutter
- lib/src/api/pocket_drs_api.dart
  - create job, poll, fetch result
- lib/src/models/api_models.dart
  - request/response DTOs with JSON (freezed/json_serializable recommended)
- lib/src/state/job_controller.dart
  - handle progress, retries, cancellation

5.3 Networking
- Use `dio` or `http`.
- Add timeouts and resumable retry strategy.
- Add user-configurable server URL in settings.


6) Testing & verification

6.1 Unit tests (Dart)
- API client serialization/deserialization
- Job polling state machine
- Rendering: existing layout tests remain

6.2 Python tests
- Video decode tests with known sample mp4
- Homography correctness tests (mirror existing Dart homography tests)
- Tracking pipeline tests with synthetic frames

6.3 End-to-end tests
- Scripted run: upload sample video → get deterministic result JSON
- Validate:
  - no crashes
  - progress reporting works
  - result schema stable


7) Security / privacy (required if online)
- If local-network demo: no auth required, but keep server on private Wi‑Fi.
- If cloud:
  - API key auth (simple header)
  - rate limiting
  - TLS required
  - delete job data after X days


8) Cleanup policy
- Remove unused offline-only code paths once server path is stable.
- Keep the existing pure-Dart analysis as fallback only if it is maintained.
- Do NOT create extra documentation files unless requested.


9) Timeline (aggressive but realistic)
- Day 1–2: define API schema + stub server + Flutter upload/poll UI
- Day 3–5: implement decode + seeded tracking on server + return track
- Day 6–8: implement calibration on server + pitch-plane mapping + top-down view
- Day 9–12: stabilize error handling + diagnostics + export artifacts
- Day 13+: marker calibration / improved tracking / 3D stretch


10) Decisions you must confirm (blocking)
- D1: Deployment target: Local laptop server only, or public cloud?
- D2: Calibration method: keep taps only for MVP, or add marker calibration now?
- D3: Tracking mode: require seed tap always (more reliable), or attempt fully automatic?
- D4: “3D physics” requirement: must-have or stretch?
